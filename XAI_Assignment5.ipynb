{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e830ff-872a-4741-bf6b-d233b9ad855f",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "H0: A CNN model trained for skin cancer detection of moles looks at other areas of the image rather than the mole itself.\n",
    "\n",
    "H1: A CNN model trained for skin cancer detection of moles looks at the mole itself rather than other areas of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba7d424-4f5f-43f0-94b9-44f5f912aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bdd3b-0a92-41df-ba8b-5362e646d817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-caching images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching images: 100%|█████████████████████| 4000/4000 [00:03<00:00, 1332.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-caching images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching images: 100%|█████████████████████| 1000/1000 [00:00<00:00, 1310.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-caching images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching images: 100%|██████████████████████| 5015/5015 [00:39<00:00, 127.05it/s]\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train 0.3668 | Val 0.0141 | Val Acc 0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training:   2%|▍                        | 2/125 [00:10<10:53,  5.32s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import kagglehub\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data setup\n",
    "path = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n",
    "df = pd.read_csv(os.path.join(path, \"HAM10000_metadata.csv\"))\n",
    "# Concept: mole (melanoma, label==1); benign lesions, label==0\n",
    "df[\"label\"] = df[\"dx\"].apply(lambda x: 1 if x == \"mel\" else 0)\n",
    "df[\"image_path\"] = df[\"image_id\"].apply(\n",
    "    lambda x: os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\")\n",
    "    if os.path.exists(os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\"))\n",
    "    else os.path.join(path, \"HAM10000_images_part_2\", f\"{x}.jpg\")\n",
    ")\n",
    "train_df = df.iloc[:5000]\n",
    "test_df = df.iloc[5000:]\n",
    "train_df, val_df = train_df.iloc[:4000], train_df.iloc[4000:]\n",
    "\n",
    "# Create a cache directory for transformed images\n",
    "cache_dir = \"processed_ham10000\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Dataset with caching and progress indication for pre-caching\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None, cache=True, cache_dir=cache_dir, precache=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        if self.cache and precache:\n",
    "            print(\"Pre-caching images...\")\n",
    "            for i in tqdm(range(len(self.df)), desc=\"Caching images\"):\n",
    "                _ = self.__getitem__(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        image_id = row[\"image_id\"]\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "        cache_path = os.path.join(self.cache_dir, f\"{image_id}.pt\")\n",
    "        if self.cache and os.path.exists(cache_path):\n",
    "            image = torch.load(cache_path, map_location=\"cpu\", weights_only=False)\n",
    "        else:\n",
    "            img_path = row[\"image_path\"]\n",
    "            image = read_image(img_path).float() / 255.0\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            if self.cache:\n",
    "                torch.save(image, cache_path)\n",
    "        return image, label\n",
    "\n",
    "# Define transform and create datasets with pre-caching enabled\n",
    "transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "train_dataset = HAM10000Dataset(train_df, transform, precache=True)\n",
    "val_dataset   = HAM10000Dataset(val_df, transform, precache=True)\n",
    "test_dataset  = HAM10000Dataset(test_df, transform, precache=True)\n",
    "\n",
    "# Create DataLoaders with progress wrapped in training loops\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pretrained ResNet-50 and freeze all layers except layer4\n",
    "base_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "for name, param in base_model.named_parameters():\n",
    "    param.requires_grad = (\"layer4\" in name)\n",
    "\n",
    "# Custom model: ResNet-50 base with GAP and a two-layer feedforward network\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(base_model.fc.in_features, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.base.conv1(x)\n",
    "        x = self.base.bn1(x)\n",
    "        x = self.base.relu(x)\n",
    "        x = self.base.maxpool(x)\n",
    "        x = self.base.layer1(x)\n",
    "        x = self.base.layer2(x)\n",
    "        x = self.base.layer3(x)\n",
    "        x = self.base.layer4(x)\n",
    "        x = self.base.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CustomResNet(base_model).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, count = 0, 0, 0\n",
    "    for imgs, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).long()\n",
    "        correct += (preds == labels.long()).sum().item()\n",
    "        count += imgs.size(0)\n",
    "    return total_loss / count, correct / count\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * imgs.size(0)\n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch+1}: Train {train_loss:.4f} | Val {val_loss:.4f} | Val Acc {val_acc:.4f}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss {test_loss:.4f} | Test Acc {test_acc:.4f}\")\n",
    "\n",
    "# Activation extractor from \"base.layer4\"\n",
    "class ActivationExtractor(nn.Module):\n",
    "    def __init__(self, model, layer_name):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layer_name = layer_name\n",
    "        self.activations = []\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.activations.append(output.detach())\n",
    "    def get_activations(self, x):\n",
    "        self.activations = []\n",
    "        handle = dict([*self.model.named_modules()])[self.layer_name].register_forward_hook(self.hook_fn)\n",
    "        _ = self.model(x)\n",
    "        handle.remove()\n",
    "        return self.activations[0]\n",
    "\n",
    "extractor = ActivationExtractor(model, \"base.layer4\")\n",
    "\n",
    "def extract_acts(dataset, indices):\n",
    "    acts = []\n",
    "    for idx in indices:\n",
    "        img, _ = dataset[idx]\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        act = extractor.get_activations(img)\n",
    "        acts.append(act.view(act.size(1), -1).mean(dim=1).cpu().numpy())\n",
    "    return np.array(acts)\n",
    "\n",
    "# TCAV: Compare concept (mole, label==1) vs. random (benign, label==0)\n",
    "N = 10\n",
    "concept_pool = train_df[train_df[\"label\"] == 1].index.tolist()\n",
    "random_pool = train_df[train_df[\"label\"] == 0].index.tolist()\n",
    "concept_sample_size = min(30, len(concept_pool))\n",
    "fixed_concept_indices = random.sample(concept_pool, concept_sample_size)\n",
    "concept_acts = extract_acts(train_dataset, fixed_concept_indices)\n",
    "\n",
    "tcav_scores = []\n",
    "for i in range(N):\n",
    "    rand_indices = random.sample(random_pool, concept_sample_size)\n",
    "    random_acts = extract_acts(train_dataset, rand_indices)\n",
    "    X = np.vstack([concept_acts, random_acts])\n",
    "    y = [1] * len(concept_acts) + [0] * len(random_acts)\n",
    "    clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "    clf.fit(X, y)\n",
    "    scores = clf.decision_function(concept_acts)\n",
    "    tcav_scores.append(np.mean(scores > 0))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot(tcav_scores)\n",
    "plt.title(\"TCAV Scores (Mole vs. Benign)\")\n",
    "plt.ylabel(\"TCAV Score\")\n",
    "plt.xticks([1], [\"TCAV\"])\n",
    "plt.show()\n",
    "\n",
    "# Visualize sample concept images (moles)\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for ax, idx in zip(axs, fixed_concept_indices[:5]):\n",
    "    img, _ = train_dataset[idx]\n",
    "    ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Sample Mole Images (Concept)\")\n",
    "plt.show()\n",
    "\n",
    "# Feature map visualization from \"base.layer4\" for a sample image\n",
    "def visualize_activation(image, layer=\"base.layer4\", num_maps=5):\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output.detach().cpu())\n",
    "    handle = dict([*model.named_modules()])[layer].register_forward_hook(hook_fn)\n",
    "    _ = model(image.unsqueeze(0).to(device))\n",
    "    handle.remove()\n",
    "    act = activations[0].squeeze(0)\n",
    "    fig, axs = plt.subplots(1, num_maps, figsize=(15, 3))\n",
    "    for i in range(num_maps):\n",
    "        axs[i].imshow(act[i].numpy(), cmap=\"viridis\")\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.suptitle(\"Feature Maps from Base.Layer4\")\n",
    "    plt.show()\n",
    "\n",
    "sample_img, _ = train_dataset[0]\n",
    "visualize_activation(sample_img, \"base.layer4\", num_maps=5)\n",
    "\n",
    "# GradCAM visualization using \"base.layer4\"\n",
    "def grad_cam(model, image, target_layer=\"base.layer4\", target_class=1):\n",
    "    model.eval()\n",
    "    activations, gradients = {}, {}\n",
    "    def forward_hook(module, input, output):\n",
    "        activations[\"value\"] = output\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        gradients[\"value\"] = grad_out[0]\n",
    "    target_module = dict([*model.named_modules()])[target_layer]\n",
    "    h1 = target_module.register_forward_hook(forward_hook)\n",
    "    h2 = target_module.register_backward_hook(backward_hook)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    output = model(image)\n",
    "    score = output if target_class == 1 else -output\n",
    "    model.zero_grad()\n",
    "    score.backward()\n",
    "    act = activations[\"value\"].detach()\n",
    "    grad = gradients[\"value\"].detach()\n",
    "    weights = grad.mean(dim=(2, 3), keepdim=True)\n",
    "    cam = torch.relu((weights * act).sum(dim=1, keepdim=True))\n",
    "    cam = torch.nn.functional.interpolate(cam, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    cam = cam.squeeze().cpu().numpy()\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "    h1.remove(); h2.remove()\n",
    "    return cam\n",
    "\n",
    "heatmap = grad_cam(model, sample_img, \"base.layer4\", target_class=1)\n",
    "img_np = sample_img.permute(1, 2, 0).cpu().numpy()\n",
    "plt.imshow(img_np)\n",
    "plt.imshow(heatmap, cmap=\"jet\", alpha=0.5)\n",
    "plt.title(\"GradCAM - Mole (Concept)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Integration for Assignment:\n",
    "#\n",
    "# H0: The pretrained ResNet-50 model (with only its last bottleneck block and a new head unfrozen)\n",
    "#     does not significantly utilize the \"mole\" concept in its predictions.\n",
    "# H1: The model's predictions are significantly influenced by the \"mole\" concept.\n",
    "#\n",
    "# Approach:\n",
    "# 1. Use a frozen ResNet-50 base (except layer4) and add a two-layer feedforward network.\n",
    "# 2. Compute TCAV scores by contrasting activations from the concept set (mole images)\n",
    "#    with those from a random set (benign lesions).\n",
    "# 3. Visualize the distribution of TCAV scores, sample concept images, internal feature maps,\n",
    "#    and GradCAM overlays to provide both quantitative and qualitative insights.\n",
    "#\n",
    "# The dataset class caches transformed images in 'processed_ham10000' and shows progress\n",
    "# during pre-caching and data loading via tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ff03c-a812-4ebf-946d-dedf04624647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbf0ca-58e4-4d59-aad3-6c68de9ccfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/Projects/XAI/Xenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import kagglehub\n",
    "import random\n",
    "\n",
    "# Data setup\n",
    "path = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n",
    "df = pd.read_csv(os.path.join(path, \"HAM10000_metadata.csv\"))\n",
    "# Concept: mole (melanoma, label==1); benign lesions, label==0\n",
    "df[\"label\"] = df[\"dx\"].apply(lambda x: 1 if x == \"mel\" else 0)\n",
    "df[\"image_path\"] = df[\"image_id\"].apply(\n",
    "    lambda x: os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\")\n",
    "    if os.path.exists(os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\"))\n",
    "    else os.path.join(path, \"HAM10000_images_part_2\", f\"{x}.jpg\")\n",
    ")\n",
    "train_df = df.iloc[:5000]\n",
    "test_df = df.iloc[5000:]\n",
    "train_df, val_df = train_df.iloc[:4000], train_df.iloc[4000:]\n",
    "\n",
    "# Create a cache directory for transformed images\n",
    "cache_dir = \"processed_ham10000\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Dataset with caching to speed up future loads\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None, cache=True, cache_dir=cache_dir):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "        self.cache_dir = cache_dir\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        image_id = row[\"image_id\"]\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "        cache_path = os.path.join(self.cache_dir, f\"{image_id}.pt\")\n",
    "        if self.cache and os.path.exists(cache_path):\n",
    "            image = torch.load(cache_path, map_location=\"cpu\", weights_only=False)  # weights_only=False is default but explicit\n",
    "        else:\n",
    "            img_path = row[\"image_path\"]\n",
    "            image = read_image(img_path).float() / 255.0\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            if self.cache:\n",
    "                torch.save(image, cache_path)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "train_dataset = HAM10000Dataset(train_df, transform)\n",
    "val_dataset = HAM10000Dataset(val_df, transform)\n",
    "test_dataset = HAM10000Dataset(test_df, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pretrained ResNet-50 and freeze all layers except the last bottleneck block (layer4)\n",
    "base_model = models.resnet50(weights=True)\n",
    "for name, param in base_model.named_parameters():\n",
    "    if \"layer4\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Custom model: ResNet-50 base with GAP and a two-layer feedforward network\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.base = base_model  # pretrained ResNet-50 backbone\n",
    "        # New head: uses the features from ResNet's GAP\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(base_model.fc.in_features, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.base.conv1(x)\n",
    "        x = self.base.bn1(x)\n",
    "        x = self.base.relu(x)\n",
    "        x = self.base.maxpool(x)\n",
    "        x = self.base.layer1(x)\n",
    "        x = self.base.layer2(x)\n",
    "        x = self.base.layer3(x)\n",
    "        x = self.base.layer4(x)\n",
    "        x = self.base.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CustomResNet(base_model).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, count = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).long()\n",
    "            correct += (preds == labels.long()).sum().item()\n",
    "            count += imgs.size(0)\n",
    "    return total_loss / count, correct / count\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * imgs.size(0)\n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch+1}: Train {train_loss:.4f} | Val {val_loss:.4f} | Val Acc {val_acc:.4f}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss {test_loss:.4f} | Test Acc {test_acc:.4f}\")\n",
    "\n",
    "# Activation extractor from the target layer (\"base.layer4\")\n",
    "class ActivationExtractor(nn.Module):\n",
    "    def __init__(self, model, layer_name):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layer_name = layer_name\n",
    "        self.activations = []\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.activations.append(output.detach())\n",
    "    def get_activations(self, x):\n",
    "        self.activations = []\n",
    "        handle = dict([*self.model.named_modules()])[self.layer_name].register_forward_hook(self.hook_fn)\n",
    "        _ = self.model(x)\n",
    "        handle.remove()\n",
    "        return self.activations[0]\n",
    "\n",
    "extractor = ActivationExtractor(model, \"base.layer4\")\n",
    "\n",
    "# Extract flattened activations (averaging spatial dimensions)\n",
    "def extract_acts(dataset, indices):\n",
    "    acts = []\n",
    "    for idx in indices:\n",
    "        img, _ = dataset[idx]\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        act = extractor.get_activations(img)  # shape: [1, C, H, W]\n",
    "        acts.append(act.view(act.size(1), -1).mean(dim=1).cpu().numpy())\n",
    "    return np.array(acts)\n",
    "\n",
    "# TCAV: Compare concept (mole, label==1) vs. random (benign, label==0)\n",
    "N = 10  # number of random trials\n",
    "concept_pool = train_df[train_df[\"label\"] == 1].index.tolist()\n",
    "random_pool = train_df[train_df[\"label\"] == 0].index.tolist()\n",
    "concept_sample_size = min(30, len(concept_pool))\n",
    "fixed_concept_indices = random.sample(concept_pool, concept_sample_size)\n",
    "concept_acts = extract_acts(train_dataset, fixed_concept_indices)\n",
    "\n",
    "tcav_scores = []\n",
    "for i in range(N):\n",
    "    rand_indices = random.sample(random_pool, concept_sample_size)\n",
    "    random_acts = extract_acts(train_dataset, rand_indices)\n",
    "    X = np.vstack([concept_acts, random_acts])\n",
    "    y = np.array([1] * len(concept_acts) + [0] * len(random_acts))\n",
    "    clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "    clf.fit(X, y)\n",
    "    scores = clf.decision_function(concept_acts)\n",
    "    tcav_score = np.mean(scores > 0)\n",
    "    tcav_scores.append(tcav_score)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot(tcav_scores)\n",
    "plt.title(\"TCAV Scores (Mole vs. Benign)\")\n",
    "plt.ylabel(\"TCAV Score\")\n",
    "plt.xticks([1], [\"TCAV\"])\n",
    "plt.show()\n",
    "\n",
    "# Visualize sample concept images (moles)\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "sample_concept_indices = fixed_concept_indices[:5]\n",
    "for ax, idx in zip(axs, sample_concept_indices):\n",
    "    img, _ = train_dataset[idx]\n",
    "    ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Sample Mole Images (Concept)\")\n",
    "plt.show()\n",
    "\n",
    "# Feature map visualization from \"base.layer4\" for a sample image\n",
    "def visualize_activation(image, layer=\"base.layer4\", num_maps=5):\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output.detach().cpu())\n",
    "    handle = dict([*model.named_modules()])[layer].register_forward_hook(hook_fn)\n",
    "    _ = model(image.unsqueeze(0).to(device))\n",
    "    handle.remove()\n",
    "    act = activations[0].squeeze(0)  # shape: [C, H, W]\n",
    "    fig, axs = plt.subplots(1, num_maps, figsize=(15, 3))\n",
    "    for i in range(num_maps):\n",
    "        axs[i].imshow(act[i].numpy(), cmap=\"viridis\")\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.suptitle(\"Feature Maps from Base.Layer4\")\n",
    "    plt.show()\n",
    "\n",
    "sample_img, _ = train_dataset[0]\n",
    "visualize_activation(sample_img, \"base.layer4\", num_maps=5)\n",
    "\n",
    "# GradCAM visualization using \"base.layer4\"\n",
    "def grad_cam(model, image, target_layer=\"base.layer4\", target_class=1):\n",
    "    model.eval()\n",
    "    activations, gradients = {}, {}\n",
    "    def forward_hook(module, input, output):\n",
    "        activations[\"value\"] = output\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        gradients[\"value\"] = grad_out[0]\n",
    "    target_module = dict([*model.named_modules()])[target_layer]\n",
    "    h1 = target_module.register_forward_hook(forward_hook)\n",
    "    h2 = target_module.register_backward_hook(backward_hook)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    output = model(image)\n",
    "    score = output if target_class == 1 else -output\n",
    "    model.zero_grad()\n",
    "    score.backward()\n",
    "    act = activations[\"value\"].detach()\n",
    "    grad = gradients[\"value\"].detach()\n",
    "    weights = grad.mean(dim=(2, 3), keepdim=True)\n",
    "    cam = torch.relu((weights * act).sum(dim=1, keepdim=True))\n",
    "    cam = torch.nn.functional.interpolate(cam, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    cam = cam.squeeze().cpu().numpy()\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "    h1.remove(); h2.remove()\n",
    "    return cam\n",
    "\n",
    "heatmap = grad_cam(model, sample_img, \"base.layer4\", target_class=1)\n",
    "img_np = sample_img.permute(1, 2, 0).cpu().numpy()\n",
    "plt.imshow(img_np)\n",
    "plt.imshow(heatmap, cmap=\"jet\", alpha=0.5)\n",
    "plt.title(\"GradCAM - Mole (Concept)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Integration for Assignment:\n",
    "#\n",
    "# H0: The pretrained ResNet-50 model (with only its last bottleneck block and a new head unfrozen)\n",
    "#     does not significantly utilize the \"mole\" concept in its predictions.\n",
    "# H1: The model's predictions are significantly influenced by the \"mole\" concept.\n",
    "#\n",
    "# Approach:\n",
    "# 1. Use a frozen ResNet-50 base (except layer4) and add a two-layer feedforward network.\n",
    "# 2. Compute TCAV scores by contrasting activations from the concept set (mole images)\n",
    "#    with those from a random set (benign lesions).\n",
    "# 3. Visualize the distribution of TCAV scores, sample concept images, internal feature maps,\n",
    "#    and GradCAM overlays to provide both quantitative and qualitative insights.\n",
    "#\n",
    "# The dataset class caches transformed images in 'processed_ham10000' for faster loading on future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e993a-246f-424f-8707-e919f57e77d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c1542-1799-4aa7-86fa-2ebc4b2c8745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /Users/ryan/.cache/kagglehub/datasets/kmader/skin-cancer-mnist-ham10000/versions/2\n",
      "Epoch 1: Train Loss 1.0060 | Val Loss 1.0641 | Val Acc 0.5820\n",
      "Epoch 2: Train Loss 0.5738 | Val Loss 0.0325 | Val Acc 0.9900\n",
      "Epoch 3: Train Loss 0.5380 | Val Loss 0.0289 | Val Acc 0.9890\n",
      "Epoch 4: Train Loss 0.4610 | Val Loss 0.1205 | Val Acc 0.9490\n",
      "Epoch 5: Train Loss 0.4126 | Val Loss 0.0033 | Val Acc 0.9990\n",
      "Test Loss 0.6973 | Test Acc 0.7093\n",
      "TCAV Score: 0.9188\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import permutation_test\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "import kagglehub\n",
    "\n",
    "# Data\n",
    "path = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n",
    "print(\"Dataset path:\", path)\n",
    "df = pd.read_csv(os.path.join(path, \"HAM10000_metadata.csv\"))\n",
    "df[\"label\"] = df[\"dx\"].apply(lambda x: 1 if x == \"mel\" else 0)\n",
    "df[\"image_path\"] = df[\"image_id\"].apply(\n",
    "    lambda x: os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\")\n",
    "    if os.path.exists(os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\"))\n",
    "    else os.path.join(path, \"HAM10000_images_part_2\", f\"{x}.jpg\")\n",
    ")\n",
    "train_df = df.iloc[:5000]\n",
    "test_df = df.iloc[5000:]\n",
    "train_df, val_df = train_df.iloc[:4000], train_df.iloc[4000:]\n",
    "\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = read_image(img_path).float() / 255.0\n",
    "        label = torch.tensor(self.df.iloc[idx][\"label\"], dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "train_dataset = HAM10000Dataset(train_df, transform)\n",
    "val_dataset = HAM10000Dataset(val_df, transform)\n",
    "test_dataset = HAM10000Dataset(test_df, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# CNN with BN and dropout\n",
    "class SkinCancerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkinCancerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(64 * 28 * 28, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SkinCancerCNN().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, count = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).long()\n",
    "            correct += (preds == labels.long()).sum().item()\n",
    "            count += imgs.size(0)\n",
    "    return total_loss / count, correct / count\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * imgs.size(0)\n",
    "        train_loss = epoch_loss / len(train_loader.dataset)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss {train_loss:.4f} | Val Loss {val_loss:.4f} | Val Acc {val_acc:.4f}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss {test_loss:.4f} | Test Acc {test_acc:.4f}\")\n",
    "\n",
    "# TCAV\n",
    "class ActivationExtractor(nn.Module):\n",
    "    def __init__(self, model, layer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layer = layer\n",
    "        self.activations = []\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.activations.append(output.detach())\n",
    "    def get_activations(self, x):\n",
    "        self.activations = []\n",
    "        handle = getattr(self.model, self.layer).register_forward_hook(self.hook_fn)\n",
    "        _ = self.model(x)\n",
    "        handle.remove()\n",
    "        return self.activations[0].cpu().numpy()\n",
    "\n",
    "extractor = ActivationExtractor(model, 'conv3')\n",
    "concept_acts, non_concept_acts = [], []\n",
    "for imgs, labels in train_loader:\n",
    "    imgs = imgs.to(device)\n",
    "    acts = extractor.get_activations(imgs)\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    for i in range(len(labels_np)):\n",
    "        if labels_np[i] == 1:\n",
    "            concept_acts.append(acts[i])\n",
    "        else:\n",
    "            non_concept_acts.append(acts[i])\n",
    "concept_acts = np.array(concept_acts).reshape(len(concept_acts), -1)\n",
    "non_concept_acts = np.array(non_concept_acts).reshape(len(non_concept_acts), -1)\n",
    "cav_model = SGDClassifier()\n",
    "cav_labels = np.array([1] * len(concept_acts) + [0] * len(non_concept_acts))\n",
    "cav_data = np.vstack((concept_acts, non_concept_acts))\n",
    "cav_model.fit(cav_data, cav_labels)\n",
    "tcav_scores = cav_model.decision_function(concept_acts)\n",
    "tcav_score = np.mean(tcav_scores > 0)\n",
    "print(f\"TCAV Score: {tcav_score:.4f}\")\n",
    "def diff_mean(a, b):\n",
    "    return np.mean(a) - np.mean(b)\n",
    "perm_result = permutation_test(\n",
    "    (concept_acts.flatten(), non_concept_acts.flatten()),\n",
    "    statistic=diff_mean,\n",
    "    vectorized=False,\n",
    "    n_resamples=1000,\n",
    "    alternative=\"greater\",\n",
    ")\n",
    "print(f\"Permutation Test p-value: {perm_result.pvalue:.4f}\")\n",
    "\n",
    "# Visualize sample concept images\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "concept_indices = train_df[train_df[\"label\"] == 1].index[:5]\n",
    "for ax, idx in zip(axs, concept_indices):\n",
    "    img, _ = train_dataset[idx]\n",
    "    ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Concept Images\")\n",
    "plt.show()\n",
    "\n",
    "# Feature map visualization\n",
    "def visualize_activation(model, image, layer=\"conv3\"):\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output.detach().cpu())\n",
    "    handle = getattr(model, layer).register_forward_hook(hook_fn)\n",
    "    _ = model(image.unsqueeze(0).to(device))\n",
    "    handle.remove()\n",
    "    act = activations[0].squeeze(0)\n",
    "    num_maps = act.shape[0]\n",
    "    fig, axs = plt.subplots(1, min(num_maps, 5), figsize=(15, 3))\n",
    "    for i in range(min(num_maps, 5)):\n",
    "        axs[i].imshow(act[i].numpy(), cmap=\"viridis\")\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.suptitle(\"Feature Maps\")\n",
    "    plt.show()\n",
    "\n",
    "sample_img, _ = train_dataset[0]\n",
    "visualize_activation(model, sample_img, \"conv3\")\n",
    "\n",
    "# GradCAM\n",
    "def grad_cam(model, image, target_layer=\"conv3\", target_class=1):\n",
    "    model.eval()\n",
    "    activations, gradients = {}, {}\n",
    "    def forward_hook(module, input, output):\n",
    "        activations[\"value\"] = output\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        gradients[\"value\"] = grad_out[0]\n",
    "    target_module = getattr(model, target_layer)\n",
    "    h1 = target_module.register_forward_hook(forward_hook)\n",
    "    h2 = target_module.register_backward_hook(backward_hook)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    output = model(image)\n",
    "    score = output if target_class == 1 else -output\n",
    "    model.zero_grad()\n",
    "    score.backward()\n",
    "    act = activations[\"value\"].detach()\n",
    "    grad = gradients[\"value\"].detach()\n",
    "    weights = grad.mean(dim=(2, 3), keepdim=True)\n",
    "    cam = torch.relu((weights * act).sum(dim=1, keepdim=True))\n",
    "    cam = torch.nn.functional.interpolate(cam, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    cam = cam.squeeze().cpu().numpy()\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "    h1.remove(); h2.remove()\n",
    "    return cam\n",
    "\n",
    "heatmap = grad_cam(model, sample_img, \"conv3\", target_class=1)\n",
    "img_np = sample_img.permute(1, 2, 0).numpy()\n",
    "plt.imshow(img_np)\n",
    "plt.imshow(heatmap, cmap=\"jet\", alpha=0.5)\n",
    "plt.title(\"GradCAM\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc55b1-cc40-4bf0-ba3f-1faba8b4e269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06042169-820b-4e1c-8f9f-6bff50e9ddf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be7443-5f53-4110-b278-9aefb7e03d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /Users/ryan/.cache/kagglehub/datasets/kmader/skin-cancer-mnist-ham10000/versions/2\n",
      "Epoch 1: Train 0.5294, Val 0.3571, Val Acc 0.8990\n",
      "Epoch 2: Train 0.4276, Val 0.2209, Val Acc 0.9350\n",
      "Epoch 3: Train 0.4117, Val 0.0379, Val Acc 0.9970\n",
      "Epoch 4: Train 0.3815, Val 0.2909, Val Acc 0.8920\n",
      "Epoch 5: Train 0.3703, Val 0.1911, Val Acc 0.9410\n",
      "Test 0.6835, Test Acc 0.6931\n",
      "TCAV Score: 0.6393146979260595\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import permutation_test\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "import kagglehub\n",
    "\n",
    "# Data setup\n",
    "path = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n",
    "print(\"Dataset path:\", path)\n",
    "df = pd.read_csv(os.path.join(path, \"HAM10000_metadata.csv\"))\n",
    "df[\"label\"] = df[\"dx\"].apply(lambda x: 1 if x == \"mel\" else 0)\n",
    "df[\"image_path\"] = df[\"image_id\"].apply(lambda x: os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\")\n",
    "    if os.path.exists(os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\"))\n",
    "    else os.path.join(path, \"HAM10000_images_part_2\", f\"{x}.jpg\"))\n",
    "train_df = df.iloc[:5000]\n",
    "test_df = df.iloc[5000:]\n",
    "train_df, val_df = train_df.iloc[:4000], train_df.iloc[4000:]\n",
    "\n",
    "# Dataset\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = read_image(img_path).float() / 255.0\n",
    "        label = torch.tensor(self.df.iloc[idx][\"label\"], dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "train_dataset = HAM10000Dataset(train_df, transform)\n",
    "val_dataset = HAM10000Dataset(val_df, transform)\n",
    "test_dataset = HAM10000Dataset(test_df, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model\n",
    "class SkinCancerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkinCancerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(64 * 28 * 28, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SkinCancerCNN().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, count = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).long()\n",
    "            correct += (preds == labels.long()).sum().item()\n",
    "            count += imgs.size(0)\n",
    "    return total_loss / count, correct / count\n",
    "\n",
    "# Training\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch+1}: Train {train_loss:.4f}, Val {val_loss:.4f}, Val Acc {val_acc:.4f}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test {test_loss:.4f}, Test Acc {test_acc:.4f}\")\n",
    "\n",
    "# TCAV\n",
    "class ActivationExtractor(nn.Module):\n",
    "    def __init__(self, model, layer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layer = layer\n",
    "        self.activations = []\n",
    "    def forward_hook(self, module, input, output):\n",
    "        self.activations.append(output.detach())\n",
    "    def get_activations(self, x):\n",
    "        self.activations = []\n",
    "        handle = getattr(self.model, self.layer).register_forward_hook(self.forward_hook)\n",
    "        _ = self.model(x)\n",
    "        handle.remove()\n",
    "        return self.activations[0].cpu().numpy()\n",
    "\n",
    "extractor = ActivationExtractor(model, 'conv3')\n",
    "concept_acts, non_concept_acts = [], []\n",
    "for imgs, labels in train_loader:\n",
    "    imgs = imgs.to(device)\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    acts = extractor.get_activations(imgs)\n",
    "    concept_acts.extend(acts[labels_np == 1])\n",
    "    non_concept_acts.extend(acts[labels_np == 0])\n",
    "concept_acts = np.array(concept_acts).reshape(len(concept_acts), -1)\n",
    "non_concept_acts = np.array(non_concept_acts).reshape(len(non_concept_acts), -1)\n",
    "\n",
    "cav_model = SGDClassifier()\n",
    "cav_labels = np.array([1] * len(concept_acts) + [0] * len(non_concept_acts))\n",
    "cav_data = np.vstack((concept_acts, non_concept_acts))\n",
    "cav_model.fit(cav_data, cav_labels)\n",
    "tcav_scores = cav_model.decision_function(concept_acts)\n",
    "tcav_score = np.mean(tcav_scores > 0)\n",
    "print(f\"TCAV Score: {tcav_score}\")\n",
    "\n",
    "def permutation_test_func(a, b):\n",
    "    return np.mean(a) - np.mean(b)\n",
    "perm_result = permutation_test((concept_acts.flatten(), non_concept_acts.flatten()),\n",
    "                               statistic=permutation_test_func, alternative='greater')\n",
    "print(f\"Permutation Test p-value: {perm_result.pvalue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e84a48-f95a-4b6e-a8a7-088fd08989a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/ryan/.cache/kagglehub/datasets/kmader/skin-cancer-mnist-ham10000/versions/2\n",
      "Epoch 1/5, Loss: 0.6931471824645996\n",
      "Epoch 2/5, Loss: 0.6931471824645996\n",
      "Epoch 3/5, Loss: 0.6931471824645996\n",
      "Epoch 4/5, Loss: 0.6931471824645996\n",
      "Epoch 5/5, Loss: 0.6931471824645996\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Get concept (mole) and non-concept (background) activations\u001b[39;00m\n\u001b[1;32m    129\u001b[0m concept_activations, non_concept_activations \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m:\n\u001b[1;32m    132\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    133\u001b[0m     activations \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mget_activations(images)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from scipy.stats import permutation_test\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# Install dependencies as needed:\n",
    "# pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version of HAM10000 dataset\n",
    "path = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Load HAM10000 metadata\n",
    "df = pd.read_csv(os.path.join(path, \"HAM10000_metadata.csv\"))\n",
    "\n",
    "# Map 'mel' as cancerous (1), others as non-cancerous (0)\n",
    "df[\"label\"] = df[\"dx\"].apply(lambda x: 1 if x == \"mel\" else 0)\n",
    "\n",
    "# Assign correct image paths while keeping original folder names\n",
    "df[\"image_path\"] = df[\"image_id\"].apply(lambda x: os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\")\n",
    "    if os.path.exists(os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\"))\n",
    "    else os.path.join(path, \"HAM10000_images_part_2\", f\"{x}.jpg\"))\n",
    "\n",
    "# Split into training and testing\n",
    "train_df = df.iloc[:5000]\n",
    "test_df = df.iloc[5000:]\n",
    "\n",
    "# Image Dataset class\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = read_image(img_path).float() / 255.0\n",
    "        label = torch.tensor(self.df.iloc[idx][\"label\"], dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "train_dataset = HAM10000Dataset(train_df, transform=transform)\n",
    "test_dataset = HAM10000Dataset(test_df, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Simple CNN Model\n",
    "class SkinCancerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkinCancerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64 * 28 * 28, 1)  # Binary classification\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Initialize model, loss function, optimizer\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SkinCancerCNN().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# TCAV: Extract activations\n",
    "class ActivationExtractor(nn.Module):\n",
    "    def __init__(self, model, layer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layer = layer\n",
    "        self.activations = []\n",
    "\n",
    "    def forward_hook(self, module, input, output):\n",
    "        self.activations.append(output.detach())\n",
    "\n",
    "    def get_activations(self, x):\n",
    "        self.activations = []\n",
    "        handle = getattr(self.model, self.layer).register_forward_hook(self.forward_hook)\n",
    "        _ = self.model(x)\n",
    "        handle.remove()\n",
    "        return self.activations[0].cpu().numpy()\n",
    "\n",
    "extractor = ActivationExtractor(model, 'conv3')\n",
    "\n",
    "# Get concept (mole) and non-concept (background) activations\n",
    "concept_activations, non_concept_activations = [], []\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    images = images.to(device)\n",
    "    activations = extractor.get_activations(images)\n",
    "    concept_activations.extend(activations[labels == 1])  # Assume 1 is 'mole'\n",
    "    non_concept_activations.extend(activations[labels == 0])  # Assume 0 is 'background'\n",
    "\n",
    "concept_activations = np.array(concept_activations).reshape(len(concept_activations), -1)\n",
    "non_concept_activations = np.array(non_concept_activations).reshape(len(non_concept_activations), -1)\n",
    "\n",
    "# Train Concept Activation Vectors (CAVs)\n",
    "cav_model = SGDClassifier()\n",
    "cav_labels = np.array([1] * len(concept_activations) + [0] * len(non_concept_activations))\n",
    "cav_data = np.vstack((concept_activations, non_concept_activations))\n",
    "cav_model.fit(cav_data, cav_labels)\n",
    "\n",
    "# Compute TCAV score\n",
    "tcav_scores = cav_model.decision_function(concept_activations)\n",
    "tcav_score = np.mean(tcav_scores > 0)\n",
    "\n",
    "print(f\"TCAV Score (higher = more reliance on concept): {tcav_score}\")\n",
    "\n",
    "# Statistical Testing (Permutation Test)\n",
    "def permutation_test_func(a, b):\n",
    "    return np.mean(a) - np.mean(b)\n",
    "\n",
    "perm_result = permutation_test((concept_activations.flatten(), non_concept_activations.flatten()),\n",
    "                               statistic=permutation_test_func, alternative='greater')\n",
    "\n",
    "print(f\"Permutation Test p-value: {perm_result.pvalue}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1385537-ced6-470f-be79-6b3faaf96dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04089c19-355b-4957-84a1-d3f6793b2e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "786f1c57-1cd5-4e08-9eb6-057f0af78fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/kmader/skin-cancer-mnist-ham10000?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5.20G/5.20G [02:15<00:00, 41.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/ryan/.cache/kagglehub/datasets/kmader/skin-cancer-mnist-ham10000/versions/2\n",
      "Epoch 1/5, Loss: 0.1766643077135086\n",
      "Epoch 2/5, Loss: 0.6814433336257935\n",
      "Epoch 3/5, Loss: 0.3563919961452484\n",
      "Epoch 4/5, Loss: 0.9468994140625\n",
      "Epoch 5/5, Loss: 0.36127737164497375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from scipy.stats import permutation_test\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# Install dependencies as needed:\n",
    "# pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version of HAM10000 dataset\n",
    "path = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Load HAM10000 metadata\n",
    "df = pd.read_csv(os.path.join(path, \"HAM10000_metadata.csv\"))\n",
    "\n",
    "# Map 'mel' as cancerous (1), others as non-cancerous (0)\n",
    "df[\"label\"] = df[\"dx\"].apply(lambda x: 1 if x == \"mel\" else 0)\n",
    "\n",
    "# Assign correct image paths while keeping original folder names\n",
    "df[\"image_path\"] = df[\"image_id\"].apply(lambda x: os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\")\n",
    "    if os.path.exists(os.path.join(path, \"HAM10000_images_part_1\", f\"{x}.jpg\"))\n",
    "    else os.path.join(path, \"HAM10000_images_part_2\", f\"{x}.jpg\"))\n",
    "\n",
    "# Split into training and testing\n",
    "train_df = df.iloc[:5000]\n",
    "test_df = df.iloc[5000:]\n",
    "\n",
    "# Image Dataset class\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = read_image(img_path).float() / 255.0\n",
    "        label = torch.tensor(self.df.iloc[idx][\"label\"], dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "train_dataset = HAM10000Dataset(train_df, transform=transform)\n",
    "test_dataset = HAM10000Dataset(test_df, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Simple CNN Model\n",
    "class SkinCancerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkinCancerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64 * 28 * 28, 2)  # Binary classification\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, optimizer\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SkinCancerCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# TCAV and Statistical Testing omitted for brevity, apply same logic with new dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3325968d-bbf4-4257-bb96-b00663f16c61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get concept (mole) and non-concept (background) activations\u001b[39;00m\n\u001b[1;32m     22\u001b[0m concept_activations, non_concept_activations \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m:\n\u001b[1;32m     25\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m     activations \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mget_activations(images)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# TCAV: Extract activations\n",
    "class ActivationExtractor(nn.Module):\n",
    "    def __init__(self, model, layer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layer = layer\n",
    "        self.activations = []\n",
    "\n",
    "    def forward_hook(self, module, input, output):\n",
    "        self.activations.append(output.detach())\n",
    "\n",
    "    def get_activations(self, x):\n",
    "        self.activations = []\n",
    "        handle = getattr(self.model, self.layer).register_forward_hook(self.forward_hook)\n",
    "        _ = self.model(x)\n",
    "        handle.remove()\n",
    "        return self.activations[0].cpu().numpy()\n",
    "\n",
    "extractor = ActivationExtractor(model, 'conv3')\n",
    "\n",
    "# Get concept (mole) and non-concept (background) activations\n",
    "concept_activations, non_concept_activations = [], []\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    images = images.to(device)\n",
    "    activations = extractor.get_activations(images)\n",
    "    concept_activations.extend(activations[labels == 1])  # Assume 1 is 'mole'\n",
    "    non_concept_activations.extend(activations[labels == 0])  # Assume 0 is 'background'\n",
    "\n",
    "concept_activations = np.array(concept_activations).reshape(len(concept_activations), -1)\n",
    "non_concept_activations = np.array(non_concept_activations).reshape(len(non_concept_activations), -1)\n",
    "\n",
    "# Train Concept Activation Vectors (CAVs)\n",
    "cav_model = SGDClassifier()\n",
    "cav_labels = np.array([1] * len(concept_activations) + [0] * len(non_concept_activations))\n",
    "cav_data = np.vstack((concept_activations, non_concept_activations))\n",
    "cav_model.fit(cav_data, cav_labels)\n",
    "\n",
    "# Compute TCAV score\n",
    "tcav_scores = cav_model.decision_function(concept_activations)\n",
    "tcav_score = np.mean(tcav_scores > 0)\n",
    "\n",
    "print(f\"TCAV Score (higher = more reliance on concept): {tcav_score}\")\n",
    "\n",
    "# Statistical Testing (Permutation Test)\n",
    "def permutation_test_func(a, b):\n",
    "    return np.mean(a) - np.mean(b)\n",
    "\n",
    "perm_result = permutation_test((concept_activations.flatten(), non_concept_activations.flatten()),\n",
    "                               statistic=permutation_test_func, alternative='greater')\n",
    "\n",
    "print(f\"Permutation Test p-value: {perm_result.pvalue}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8d567-4b8c-417a-85f0-28bc2b3edab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
